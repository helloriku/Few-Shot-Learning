{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook I reproduced the prototype network from https://arxiv.org/abs/1703.05175 by training and testing it on the omniglot dataset.\nThe analysis done here is not exactly the same as in the paper. However it still shows very good results even though this notebook is a reduced form of the paper.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn, cdist\nfrom torchvision import datasets, transforms\nimport helper\nfrom skimage import io\nimport os\nfrom sklearn.model_selection import train_test_split\nimport random\nimport numpy as np\nnp.set_printoptions(precision=5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(),transforms.Resize(28),\n                                ])","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#get all paths, to sample randomly later\npaths=['../input/omniglot/images_background/','../input/omniglot/images_evaluation/']\nclasses = []\n\nfor path in paths:\n    for (dirpath, dirnames, filenames) in os.walk(path):\n        alphabets = dirnames\n        break\n\n    for alphabet in alphabets:\n        for (dirpath, dirnames, filenames) in os.walk(path+alphabet):\n            for element in dirnames:\n                classes.append(path+alphabet+'/'+element)\n            break\n\nprint(len(classes))","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1623\n","output_type":"stream"}]},{"cell_type":"code","source":"# split classes into training and testing sets\ntrain_classes, test_classes = train_test_split(classes, random_state=1, train_size=0.86)\nprint(len(train_classes))\n","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1395\n","output_type":"stream"}]},{"cell_type":"code","source":"classes[:5]","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['../input/omniglot/images_background/Grantha/character15',\n '../input/omniglot/images_background/Grantha/character11',\n '../input/omniglot/images_background/Grantha/character35',\n '../input/omniglot/images_background/Grantha/character21',\n '../input/omniglot/images_background/Grantha/character38']"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device))\n\nclass ProtoNet(nn.Module):\n    def __init__(self):\n        super(ProtoNet, self).__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n            nn.ReLU()\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n            nn.ReLU()\n        )\n        self.block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n            nn.ReLU()\n        )\n        self.block_4 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'),\n            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n            nn.ReLU()\n        )\n        \n\n    def forward(self, x):\n        x = self.block_1(x)\n        x = self.block_2(x)\n        x = self.block_3(x)\n        #x = self.block_4(x)\n        x = torch.flatten(x, start_dim=1, end_dim=-1) # 展开为1维\n        return x\n    \nmodel = ProtoNet().to(device)\n","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"# now the actual training algorithm (algorithm 1 in the paper)\n\nNc = 60\nN = 20\nNs = 5 # support\nNq = N - Ns # query\nnorm = 1/(Nc*Nq)\nloss_function = nn.LogSoftmax(dim=0)\nnum_episodes = 700\nprint_interval = 10\nrecent_loss = []\nrecent_accuracy = []\n\nfor e in range(1,num_episodes+1):\n    # samble Nc classes\n    episode_classes = random.sample(train_classes, Nc) # 抽取60个类训练\n    prototypes = [] # 每个类的原型\n    query_tensors = [] # 所有类的查询集\n    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n    for k in range(Nc):\n        #load entire folder\n\n        support = []\n        query = []\n\n        for (dirpath, dirnames, filenames) in os.walk(episode_classes[k]):\n            for count, filename in enumerate(filenames,0):\n                if not filename.endswith('.png'): continue\n                image = transform(io.imread(episode_classes[k]+'/'+filename))\n                if count < Ns:\n                    support.append(image) # 前5个样本作为支撑集\n                else:\n                    query.append(image)   # 后15个作为查询集\n        \n        support_tensor = torch.stack(support)  # (5, 1, 28, 28)\n        # query是 [15个shape(1,28,28)], 用stack可以把query中的15个 (1,28,28) tensor 合并为1个(15, 1, 28, 28)tensor\n        query_tensor = torch.stack(query) # [15, 1, 28, 28]， 当前类\n        query_tensors.append(query_tensor) # 所有类\n\n        # now that we have our support and query sets\n        # compute the prototypes with the support set\n\n        prototypes.append(torch.mean(model(support_tensor.to(device)), axis=0))  # 支撑集上当前类的平均embedding\n    \n#     print(prototypes[0].shape) # [64]\n    prototypes_tensor = torch.stack(prototypes).unsqueeze(0) # [15个[64]] --> (15, 64) --> (1, 15, 64)\n\n    query_tensors = torch.stack(query_tensors) #shape = Nc, Nq, 1, 28, 28， 即所有类的query_tensor\n#     print(query_tensors.shape) # [60, 15, 1, 28, 28]\n\n    opt.zero_grad()\n    loss = 0\n    accuracy = 0\n    for k in range(Nc):\n        \n        query_set = query_tensors[k]\n\n        embedded_vectors = model(query_set.to(device)).unsqueeze(0) # shape = 1, Nq, 64\n        distances = -cdist(prototypes_tensor, embedded_vectors).view(Nc,Nq) # 计算两两距离,https://blog.csdn.net/weixin_43509698/article/details/111463091\n#         print(distances.shape) # [60, 15]，负值\n#         print(distances[0])\n        logsoftmax_dist = loss_function(distances) # 负值\n        class_ls_dist =logsoftmax_dist[k]\n        loss -= torch.sum(class_ls_dist) # class_ls_dist为负值\n        \n        #additional, calculate accuracy on training query images\n        dist = distances.detach().to('cpu').numpy()\n        \n        pred_class = np.argmax(dist, axis=0)\n        correct = [p == k for p in pred_class]\n        accuracy += np.sum(correct)\n        \n    loss = loss*norm\n    accuracy = accuracy*norm\n\n    loss.backward()\n    opt.step()\n    \n    recent_loss.append(loss.detach().to('cpu').numpy())\n    recent_accuracy.append(accuracy)\n    \n    if e%print_interval == 0:\n        print('episode', e, 'loss', np.mean(recent_loss), 'accuracy', np.mean(recent_accuracy))\n        recent_loss = []\n        recent_accuracy = []\n","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"episode 10 loss 2.9751658 accuracy 0.3857777777777778\nepisode 20 loss 2.7927127 accuracy 0.43133333333333335\nepisode 30 loss 2.5544593 accuracy 0.4845555555555555\nepisode 40 loss 2.448113 accuracy 0.5241111111111111\nepisode 50 loss 2.3549953 accuracy 0.5422222222222223\nepisode 60 loss 2.2702096 accuracy 0.5617777777777777\nepisode 70 loss 2.2168486 accuracy 0.5835555555555556\nepisode 80 loss 2.1074445 accuracy 0.6088888888888888\nepisode 90 loss 2.0663075 accuracy 0.6152222222222221\nepisode 100 loss 2.0019755 accuracy 0.6277777777777778\nepisode 110 loss 1.9944127 accuracy 0.6278888888888889\nepisode 120 loss 1.926268 accuracy 0.6446666666666667\nepisode 130 loss 1.9099905 accuracy 0.6445555555555555\nepisode 140 loss 1.8513796 accuracy 0.6648888888888889\nepisode 150 loss 1.8053716 accuracy 0.6721111111111111\nepisode 160 loss 1.7575285 accuracy 0.6795555555555556\nepisode 170 loss 1.736083 accuracy 0.681\nepisode 180 loss 1.6978697 accuracy 0.6968888888888889\nepisode 190 loss 1.6559618 accuracy 0.7046666666666667\nepisode 200 loss 1.6547245 accuracy 0.7037777777777777\nepisode 210 loss 1.6063433 accuracy 0.7162222222222222\nepisode 220 loss 1.548573 accuracy 0.7204444444444444\nepisode 230 loss 1.5339568 accuracy 0.7274444444444444\nepisode 240 loss 1.540488 accuracy 0.7195555555555556\nepisode 250 loss 1.4964703 accuracy 0.7275555555555555\nepisode 260 loss 1.4461615 accuracy 0.7413333333333334\nepisode 270 loss 1.4125879 accuracy 0.7524444444444445\nepisode 280 loss 1.4285259 accuracy 0.7469999999999999\nepisode 290 loss 1.3804095 accuracy 0.7583333333333333\nepisode 300 loss 1.351115 accuracy 0.7577777777777778\nepisode 310 loss 1.3416961 accuracy 0.7595555555555555\nepisode 320 loss 1.280803 accuracy 0.7746666666666666\nepisode 330 loss 1.2602202 accuracy 0.776\nepisode 340 loss 1.2540295 accuracy 0.7822222222222222\nepisode 350 loss 1.2310189 accuracy 0.7901111111111111\nepisode 360 loss 1.1982855 accuracy 0.7846666666666666\nepisode 370 loss 1.1572934 accuracy 0.7992222222222222\nepisode 380 loss 1.1910787 accuracy 0.784888888888889\nepisode 390 loss 1.1854963 accuracy 0.7868888888888889\nepisode 400 loss 1.1606243 accuracy 0.7882222222222223\nepisode 410 loss 1.1036747 accuracy 0.8045555555555556\nepisode 420 loss 1.1678207 accuracy 0.7826666666666666\nepisode 430 loss 1.0747793 accuracy 0.8112222222222222\nepisode 440 loss 1.0521127 accuracy 0.8065555555555555\nepisode 450 loss 1.0707417 accuracy 0.8148888888888889\nepisode 460 loss 1.0565906 accuracy 0.8092222222222223\nepisode 470 loss 1.0182503 accuracy 0.820888888888889\nepisode 480 loss 1.0134032 accuracy 0.8312222222222221\nepisode 490 loss 0.99774396 accuracy 0.8241111111111111\nepisode 500 loss 0.94406927 accuracy 0.8408888888888889\nepisode 510 loss 0.9553485 accuracy 0.8321111111111111\nepisode 520 loss 0.9769794 accuracy 0.8202222222222222\nepisode 530 loss 0.97541654 accuracy 0.828\nepisode 540 loss 0.9518841 accuracy 0.8292222222222222\nepisode 550 loss 0.9388045 accuracy 0.8297777777777778\nepisode 560 loss 0.91353863 accuracy 0.8357777777777777\nepisode 570 loss 0.8686681 accuracy 0.8423333333333334\nepisode 580 loss 0.862189 accuracy 0.8435555555555556\nepisode 590 loss 0.8481223 accuracy 0.8498888888888889\nepisode 600 loss 0.8319171 accuracy 0.8555555555555555\nepisode 610 loss 0.8217701 accuracy 0.8560000000000001\nepisode 620 loss 0.8319194 accuracy 0.8555555555555555\nepisode 630 loss 0.83318985 accuracy 0.8508888888888888\nepisode 640 loss 0.8360383 accuracy 0.849\nepisode 650 loss 0.8118579 accuracy 0.8616666666666667\nepisode 660 loss 0.7822648 accuracy 0.8617777777777779\nepisode 670 loss 0.8012746 accuracy 0.854111111111111\nepisode 680 loss 0.7730326 accuracy 0.8594444444444443\nepisode 690 loss 0.7774152 accuracy 0.8582222222222222\nepisode 700 loss 0.7696569 accuracy 0.8613333333333333\n","output_type":"stream"}]},{"cell_type":"code","source":"# validate the trained network\n\nNc = 20\nN = 20\nNs = 5\nNq = N - Ns\nnorm = 1/(Nc*Nq)\n\nnum_episodes = 100\nprint_interval = 10\nrecent_loss = []\nrecent_accuracy = []\ntotal_loss = []\ntotal_accuracy = []\n\nfor e in range(1,num_episodes+1):\n    # samble Nc classes\n    episode_classes = random.sample(test_classes, Nc)\n    prototypes = []\n    query_tensors = []\n\n    for k in range(Nc):\n        #load entire folder\n\n        support = []\n        query = []\n\n        for (dirpath, dirnames, filenames) in os.walk(episode_classes[k]):\n            for count, filename in enumerate(filenames,0):\n                if not filename.endswith('.png'): continue\n                image = transform(io.imread(episode_classes[k]+'/'+filename))\n                if count < Ns:\n                    support.append(image)\n                else:\n                    query.append(image)\n\n        support_tensor = torch.stack(support)\n        query_tensor = torch.stack(query)\n        query_tensors.append(query_tensor)\n\n        # now that we have our support and query sets\n        # compute the prototypes with the support set\n\n        prototypes.append(torch.mean(model(support_tensor.to(device)), axis=0))\n\n    prototypes_tensor = torch.stack(prototypes).unsqueeze(0) # shape = 1, Nc, 64\n\n    query_tensors = torch.stack(query_tensors) #shape = Nc, Nq, 1, 28, 28\n\n    loss = 0\n    accuracy = 0\n    for k in range(Nc):\n        \n        query_set = query_tensors[k]\n\n        embedded_vectors = model(query_set.to(device)).unsqueeze(0) # shape = 1, Nq, 64\n        distances = -cdist(prototypes_tensor, embedded_vectors).view(Nc,Nq)\n        logsoftmax_dist = loss_function(distances)\n        class_ls_dist =logsoftmax_dist[k]\n        loss -= torch.sum(class_ls_dist)\n        \n        #calculate accuracy query images\n        dist = distances.detach().to('cpu').numpy()\n        \n        pred_class = np.argmax(dist, axis=0)\n        correct = [p == k for p in pred_class]\n        accuracy += np.sum(correct)\n        \n    loss = loss*norm\n    accuracy = accuracy*norm\n    \n    recent_loss.append(loss.detach().to('cpu').numpy())\n    recent_accuracy.append(accuracy)\n    \n    if e%print_interval == 0:\n        loss = np.mean(recent_loss)\n        accuracy = np.mean(recent_accuracy)\n        print('episode', e, 'loss', loss, 'accuracy', accuracy)\n        recent_loss = []\n        recent_accuracy = []\n        total_loss.append(loss)\n        total_accuracy.append(accuracy)\n    \nprint('total loss:', np.mean(total_loss), 'total accuracy:', np.mean(total_accuracy))","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"episode 10 loss 0.62443894 accuracy 0.8470000000000001\nepisode 20 loss 0.60585415 accuracy 0.8593333333333334\nepisode 30 loss 0.5644263 accuracy 0.8753333333333334\nepisode 40 loss 0.64522886 accuracy 0.8523333333333334\nepisode 50 loss 0.6055968 accuracy 0.8503333333333334\nepisode 60 loss 0.5934883 accuracy 0.8630000000000001\nepisode 70 loss 0.5951405 accuracy 0.8633333333333335\nepisode 80 loss 0.5802604 accuracy 0.8696666666666667\nepisode 90 loss 0.6307229 accuracy 0.8510000000000002\nepisode 100 loss 0.6509991 accuracy 0.837\ntotal loss: 0.6096156 total accuracy: 0.8568333333333333\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model was trained with 60 classes per episode and tested with 20 classes per episode. We can see that the model gives pretty good predictions even though I just used 700 training episodes while the paper uses much more. More training and also using more classes by rotationg the training images would most likely lead to even better results. However my goal was to understand few-shot and one-shot learning therefore this toy example is already enough and could be easily extended to the full analysis from the paper.\n\nIn case you have any suggestions to improve on this notebook I would be happy if you would write a comment.","metadata":{}}]}